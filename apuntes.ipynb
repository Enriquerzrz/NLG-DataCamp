{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Natural Lenaguage Generation in Python - DataCamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 1\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Insert a tab in front of all the names\r\n",
    "names_df['input'] = names_df['input'].apply(lambda x : \"\\t\" + x)\r\n",
    "\r\n",
    "# Append a newline at the end of every name\r\n",
    "# We already appended a tab in front, so the target word should start at index 1\r\n",
    "names_df['target'] = names_df['input'].apply(lambda x : x[1:len(x)] + \"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the vocabulary\r\n",
    "vocabulary = get_vocabulary(names_df['input'])\r\n",
    "\r\n",
    "# Sort the vocabulary\r\n",
    "vocabulary_sorted = sorted(vocabulary)\r\n",
    "\r\n",
    "# Create the mapping of the vocabulary chars to integers\r\n",
    "char_to_idx = { char : idx for idx, char in enumerate(vocabulary_sorted) }\r\n",
    "\r\n",
    "# Create the mapping of the integers to vocabulary chars\r\n",
    "idx_to_char = { idx : char for idx, char in enumerate(vocabulary_sorted) }\r\n",
    "\r\n",
    "# Print the dictionaries\r\n",
    "print(char_to_idx)\r\n",
    "print(idx_to_char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the length of longest name\r\n",
    "max_len = get_max_len(names_df['input'])\r\n",
    "\r\n",
    "# Initialize the input vector\r\n",
    "input_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')\r\n",
    "\r\n",
    "# Initialize the target vector\r\n",
    "target_data = np.zeros((len(names_df['input']), max_len+1, len(vocabulary)), dtype='float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Iterate for each name in the dataset\r\n",
    "for n_idx, name in enumerate(names_df['input']):\r\n",
    "  # Iterate over each character and convert it to a one-hot encoded vector\r\n",
    "  for c_idx, char in enumerate(name):\r\n",
    "    input_data[n_idx, c_idx, char_to_idx[char]] = 1\r\n",
    "\r\n",
    "# Iterate for each name in the dataset\r\n",
    "for n_idx, name in enumerate(names_df['target']):\r\n",
    "  # Iterate over each character and convert it to a one-hot encoded vector\r\n",
    "  for c_idx, char in enumerate(name):\r\n",
    "    target_data[n_idx, c_idx, char_to_idx[char]] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a Sequential model\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# Add SimpleRNN layer of 50 units\r\n",
    "model.add(SimpleRNN(50, input_shape=(max_len+1, len(vocabulary)), return_sequences=True))\r\n",
    "\r\n",
    "# Add a TimeDistributed Dense layer of size same as the vocabulary\r\n",
    "model.add(TimeDistributed(Dense(len(vocabulary), activation='softmax')))\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\n",
    "\r\n",
    "# Print the model summary\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit the model for 5 epochs using a batch size of 128 \r\n",
    "model.fit(input_data, target_data, batch_size=128, epochs=5)\r\n",
    "\r\n",
    "# Create a 3-D zero vector and initialize it with the start token\r\n",
    "output_seq = np.zeros((1, max_len+1, len(vocabulary)))\r\n",
    "output_seq[0, 0, char_to_idx['\\t']] = 1\r\n",
    "\r\n",
    "# Get the probabilities for the first character\r\n",
    "probs = model.predict_proba(output_seq, verbose=0)[:,1,:]\r\n",
    "\r\n",
    "# Sample vocabulary to get first character\r\n",
    "first_char = np.random.choice(sorted(vocabulary), replace=False, p=probs.reshape(len(vocabulary)))\r\n",
    "\r\n",
    "# Print the character generated\r\n",
    "print(first_char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a sequential model\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# Create a dense layer of 12 units\r\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\r\n",
    "\r\n",
    "# Create a dense layer of 8 units\r\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\r\n",
    "\r\n",
    "# Create a dense layer of 1 unit\r\n",
    "model.add(Dense(1, init=\"uniform\", activation=\"sigmoid\"))\r\n",
    "\r\n",
    "# Compile the model and get gradients\r\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\r\n",
    "gradients = backend.gradients(model.output, model.trainable_weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dummy input vector\r\n",
    "input_vector = np.random.random((1,8))\r\n",
    "\r\n",
    "# Create a tensorflow session to run the network\r\n",
    "sess = tf.InteractiveSession()\r\n",
    "\r\n",
    "# Initialize all the variables\r\n",
    "sess.run(tf.global_variables_initializer())\r\n",
    "\r\n",
    "# Evaluate the gradients using the training examples\r\n",
    "evaluated_gradients = sess.run(gradients,feed_dict={model.input:input_vector})\r\n",
    "\r\n",
    "# Print gradient values from third layer and two nodes of the second layer\r\n",
    "print(evaluated_gradients[4])\r\n",
    "print(evaluated_gradients[2][4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the vocabulary\r\n",
    "vocabulary = sorted(set(text))\r\n",
    "\r\n",
    "# Print the vocabulary size\r\n",
    "print('Vocabulary size:', len(vocabulary))\r\n",
    "\r\n",
    "# Dictionary to save the mapping from char to integer\r\n",
    "char_to_idx = { char : idx for idx, char in enumerate(vocabulary) }\r\n",
    "\r\n",
    "# Dictionary to save the mapping from integer to char\r\n",
    "idx_to_char = { idx : char for idx, char in enumerate(vocabulary) }\r\n",
    "\r\n",
    "# Print char_to_idx and idx_to_char\r\n",
    "print(char_to_idx)\r\n",
    "print(idx_to_char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create empty lists for input and target dataset\r\n",
    "input_data = []\r\n",
    "target_data = []\r\n",
    "\r\n",
    "# Iterate to get all substrings of length maxlen\r\n",
    "for i in range(0, len(text) - maxlen):\r\n",
    "    # Find the sequence of length maxlen starting at i\r\n",
    "    input_data.append(text[i : i + maxlen])\r\n",
    "    \r\n",
    "    # Find the next char after this sequence \r\n",
    "    target_data.append(text[i + maxlen])\r\n",
    "\r\n",
    "# Print number of sequences in input data\r\n",
    "print('No of Sequences:', len(input_data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a 3-D zero vector to contain the encoded input sequences\r\n",
    "x = np.zeros((len(input_data), maxlen, len(vocabulary)), dtype='float32')\r\n",
    "\r\n",
    "# Create a 2-D zero vector to contain the encoded target characters\r\n",
    "y = np.zeros((len(target_data), len(vocabulary)), dtype='float32')\r\n",
    "\r\n",
    "# Iterate over the sequences\r\n",
    "for s_idx, sequence in enumerate(input_data):\r\n",
    "    # Iterate over all characters in the sequence\r\n",
    "    for idx, char in enumerate(sequence):\r\n",
    "        # Fill up vector x\r\n",
    "        x[s_idx, idx, char_to_idx[char]] = 1    \r\n",
    "    # Fill up vector y\r\n",
    "    y[s_idx, char_to_idx[target_data[s_idx]]] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Sequential model \r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# Add an LSTM layer of 128 units\r\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\r\n",
    "\r\n",
    "# Add a Dense output layer\r\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\r\n",
    "\r\n",
    "# Print model summary\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 3\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Sequential model \r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# Add an LSTM layer of 128 units\r\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(vocabulary))))\r\n",
    "\r\n",
    "# Add a Dense output layer\r\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
    "\r\n",
    "# Fit the model\r\n",
    "model.fit(x, y, batch_size=64, epochs=1, validation_split=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Input sequence\r\n",
    "sentence = \"that, poor contempt, or claim'd thou sle\"\r\n",
    "\r\n",
    "# Create a 3-D zero vector to contain the encoding of sentence.\r\n",
    "X_test = np.zeros((1, maxlen, len(vocabulary)))\r\n",
    "\r\n",
    "# Iterate over each character and convert them to one-hot encoded vector.\r\n",
    "for s_idx, char in enumerate(sentence):\r\n",
    "    X_test[0, s_idx, char_to_idx[char]] = 1\r\n",
    "    \r\n",
    "# Get the probability distribution using model predict\r\n",
    "preds = model.predict(X_test, verbose=0)\r\n",
    "\r\n",
    "# Get the probability distribution for the first character after the sequence\r\n",
    "preds_next_char = preds[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the probability distribution of next character\r\n",
    "preds = model.predict(X_test, verbose=0)[0]\r\n",
    "\r\n",
    "# Get the index of the most probable next character\r\n",
    "next_index = np.argmax(preds)\r\n",
    "\r\n",
    "# Map the index to the actual character and print it\r\n",
    "next_char = idx_to_char[next_index]\r\n",
    "\r\n",
    "# Print the next character\r\n",
    "print(next_char)\r\n",
    "\r\n",
    "# Input sequence and generate text\r\n",
    "sentence = \"that, poor contempt, or claim'd thou sle\"\r\n",
    "generate_text(sentence, 500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Consider only the first 50 lines of the dataset\r\n",
    "for i in range(50):\r\n",
    "\t# Split each line into two at the tab character\r\n",
    "    eng_fra_line = str(lines[i]).split(\"\\t\")\r\n",
    "    \r\n",
    "    # Separate out the English sentence \r\n",
    "    eng_line = eng_fra_line[0]\r\n",
    "    \r\n",
    "    # Append the start and end token to each French sentence\r\n",
    "    fra_line = '\\t' + eng_fra_line[1] + '\\n'\r\n",
    "    \r\n",
    "    # Append the English and French sentence to the list of sentences\r\n",
    "    english_sentences.append(eng_line)\r\n",
    "    french_sentences.append(fra_line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an empty set to contain the English vocabulary \r\n",
    "english_vocab = set()\r\n",
    "\r\n",
    "# Iterate over each English sentence\r\n",
    "for eng_line in english_sentences:\r\n",
    "  \r\n",
    "    # Convert the English line to a set\r\n",
    "    eng_line_set = set(eng_line)\r\n",
    "    \r\n",
    "    # Update English vocabulary with new characters from this line.\r\n",
    "    english_vocab = english_vocab.union(eng_line_set)\r\n",
    "\r\n",
    "# Sort the vocabulary\r\n",
    "english_vocab = sorted(list(english_vocab))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create an empty set to contain the French vocabulary \r\n",
    "french_vocab = set()\r\n",
    "\r\n",
    "# Iterate over each French sentence\r\n",
    "for fra_line in french_sentences:\r\n",
    "  \r\n",
    "    # Convert the French line to a set\r\n",
    "    fra_line_set = set(fra_line)\r\n",
    "    \r\n",
    "    # Update French vocabulary with new characters from this line.\r\n",
    "    french_vocab = french_vocab.union(fra_line_set)\r\n",
    "\r\n",
    "# Sort the vocabulary\r\n",
    "french_vocab = sorted(list(french_vocab))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dictionary to contain the character to integer mapping for English\r\n",
    "eng_char_to_idx = dict((char, idx) for idx, char in enumerate(english_vocab))\r\n",
    "\r\n",
    "# Dictionary to contain the integer to character mapping for English\r\n",
    "eng_idx_to_char = dict((idx, char) for idx, char in enumerate(english_vocab))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dictionary to contain the character to integer mapping for French\r\n",
    "fra_char_to_idx = dict((char, idx) for idx, char in enumerate(french_vocab))\r\n",
    "\r\n",
    "# Dictionary to contain the integer to character mapping for French\r\n",
    "fra_idx_to_char = dict((idx, char) for idx, char in enumerate(french_vocab))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the length of the longest English sentence\r\n",
    "max_len_eng_sent = max([len(sentence) for sentence in english_sentences])\r\n",
    "\r\n",
    "# Find the length of the longest French sentence\r\n",
    "max_len_fra_sent = max([len(sentence) for sentence in french_sentences])\r\n",
    "\r\n",
    "# Create a 3-D zero vector for the input English data\r\n",
    "eng_input_data = np.zeros((len(english_sentences), max_len_eng_sent, len(english_vocab)), dtype='float32')\r\n",
    "\r\n",
    "# Create a 3-D zero vector for the input French data\r\n",
    "fra_input_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')\r\n",
    "\r\n",
    "# Create the target vector\r\n",
    "target_data = np.zeros((len(french_sentences), max_len_fra_sent, len(french_vocab)), dtype='float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Iterate over the 50 sentences\r\n",
    "for i in range(50):\r\n",
    "    # Iterate over each English character of each sentence\r\n",
    "    for k, ch in enumerate(english_sentences[i]):\r\n",
    "        # Convert the character to one-hot encoded vector\r\n",
    "        eng_input_data[i, k, eng_char_to_idx[ch]] = 1.\r\n",
    "    \r\n",
    "    # Iterate over each French character of each sentence\r\n",
    "    for k, ch in enumerate(french_sentences[i]):\r\n",
    "        # Convert the character to one-hot encoded vector\r\n",
    "        fra_input_data[i, k, fra_char_to_idx[ch]] = 1.\r\n",
    "\r\n",
    "        # Target data will be one timestep ahead and excludes start character\r\n",
    "        if k > 0:\r\n",
    "            target_data[i, k-1, fra_char_to_idx[ch]] = 1."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create input layer\r\n",
    "encoder_input = Input(shape=(None, len(english_vocab)))\r\n",
    "\r\n",
    "# Create LSTM Layer of size 256\r\n",
    "encoder_LSTM = LSTM(256, return_state = True)\r\n",
    "\r\n",
    "# Save encoder output, hidden and cell state\r\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\r\n",
    "\r\n",
    "# Save encoder states\r\n",
    "encoder_states = [encoder_h, encoder_c]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create decoder input layer\r\n",
    "decoder_input = Input(shape=(None, len(french_vocab)))\r\n",
    "\r\n",
    "# Create LSTM layer of size 256\r\n",
    "decoder_LSTM = LSTM(256, return_sequences=True, return_state = True)\r\n",
    "\r\n",
    "# Save decoder output\r\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\r\n",
    "\r\n",
    "# Create a Dense layer with softmax activation\r\n",
    "decoder_dense = Dense(len(french_vocab), activation='softmax')\r\n",
    "\r\n",
    "# Save the decoder output\r\n",
    "decoder_out = decoder_dense(decoder_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build model\r\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\r\n",
    "\r\n",
    "# Print model summary\r\n",
    "model.summary()\r\n",
    "\r\n",
    "# Fit the model\r\n",
    "model.fit(x=[eng_input_data, fra_input_data], y=target_data,\r\n",
    "          \t\tbatch_size=64, epochs=1, validation_split=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create encoder inference model\r\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\r\n",
    "\r\n",
    "# Create decoder input states for inference\r\n",
    "decoder_state_input_h = Input(shape=(256,))\r\n",
    "decoder_state_input_c = Input(shape=(256,))\r\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\r\n",
    "\r\n",
    "# Create decoder output states for inference\r\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\r\n",
    "decoder_states = [decoder_h , decoder_c]\r\n",
    "\r\n",
    "# Create decoder dense layer\r\n",
    "decoder_out = decoder_dense(decoder_out)\r\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get encoder internal state by passing a sentence as input\r\n",
    "inp_seq = eng_input_data[0:1]\r\n",
    "states_val = encoder_model_inf.predict(inp_seq)\r\n",
    "\r\n",
    "# Seed the first character and get output from the decoder \r\n",
    "target_seq = np.zeros((1, 1, len(french_vocab)))\r\n",
    "target_seq[0, 0, fra_char_to_idx['\\t']] = 1  \r\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\r\n",
    "\r\n",
    "# Find out the next character from the Decoder output\r\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\r\n",
    "sampled_fra_char = fra_idx_to_char[max_val_index]\r\n",
    "\r\n",
    "# Print the first character predicted by the decoder\r\n",
    "print(sampled_fra_char[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill up target seq with the new char generated \r\n",
    "target_seq = np.zeros((1, 1, len(french_vocab)))\r\n",
    "target_seq[0, 0, max_val_index] = 1\r\n",
    "\r\n",
    "# Get decoder final states from last time\r\n",
    "states_val = [decoder_h, decoder_c]\r\n",
    "\r\n",
    "# Generate the next character\r\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\r\n",
    "\r\n",
    "# Map the prediction to char and print it\r\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\r\n",
    "sampled_fra_char = fra_idx_to_char[max_val_index]\r\n",
    "\r\n",
    "print(sampled_fra_char[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate 10 French sentences from inp_seq\r\n",
    "for seq_index in range(10):\r\n",
    "  \r\n",
    "    # Get next encoded english sentence\r\n",
    "    inp_seq = eng_input_data[seq_index:seq_index+1]\r\n",
    "    \r\n",
    "    # Get the translated sentence\r\n",
    "    translated_sent = translate_eng_sentence(inp_seq)\r\n",
    "    \r\n",
    "    # Print the original English sentence\r\n",
    "    print('English sentence:', english_sentences[seq_index])\r\n",
    "    \r\n",
    "    # Print the translated French sentence\r\n",
    "    print('French sentence:', translated_sent)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Capítulo 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 1\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Empty lists to store the prefixes and the suffixes\r\n",
    "prefix_sentences = []\r\n",
    "suffix_sentences = []\r\n",
    "\r\n",
    "# Create one prefix and one suffix at each character of each email\r\n",
    "for email in corpus:\r\n",
    "    for index in range(len(email)):\r\n",
    "        # Find the prefix and suffix\r\n",
    "        prefix = email[: index+1]\r\n",
    "        suffix = '\\t' + email[index+1 :] + '\\n'\r\n",
    "        \r\n",
    "        # Add the prefix and suffix to the list of prefix and suffix sentences\r\n",
    "        prefix_sentences.append(prefix)\r\n",
    "        suffix_sentences.append(suffix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize vocabulary with the start and end token\r\n",
    "vocabulary = set(['\\t', '\\n'])\r\n",
    "\r\n",
    "# Iterate for each char in each email\r\n",
    "for email in corpus:\r\n",
    "    for char in email:\r\n",
    "        # Add the char if not in vocabulary, \r\n",
    "        if (char not in vocabulary):\r\n",
    "            vocabulary.add(char)            \r\n",
    "# Sort the vocabulary\r\n",
    "vocabulary = sorted(vocabulary)\r\n",
    "\r\n",
    "# Create char to int and int to char mapping\r\n",
    "char_to_idx = dict((char, idx) for idx, char in enumerate(vocabulary))\r\n",
    "idx_to_char = dict((idx, char) for idx, char in enumerate(vocabulary))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Find the length of the longest prefix\r\n",
    "max_len_prefix_sent = max([len(prefix) for prefix in prefix_sentences])\r\n",
    "\r\n",
    "# Find the length of the longest suffix\r\n",
    "max_len_suffix_sent = max([len(suffix) for suffix in suffix_sentences])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define a 3-D zero vector for the prefix sentences\r\n",
    "input_data_prefix = np.zeros((len(prefix_sentences), max_len_prefix_sent, \r\n",
    "                              len(vocabulary)), dtype='float32')\r\n",
    "\r\n",
    "# Define a 3-D zero vector for the suffix sentences\r\n",
    "input_data_suffix = np.zeros((len(suffix_sentences), max_len_suffix_sent, \r\n",
    "                              len(vocabulary)), dtype='float32')\r\n",
    "\r\n",
    "# Define a 3-D zero vector for the target data\r\n",
    "target_data = np.zeros((len(suffix_sentences), max_len_suffix_sent, \r\n",
    "                        len(vocabulary)), dtype='float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(len(prefix_sentences)):\r\n",
    "    # Iterate over each character in each prefix\r\n",
    "    for k, ch in enumerate(prefix_sentences[i]):\r\n",
    "        # Convert the character to a one-hot encoded vector\r\n",
    "        input_data_prefix[i, k, char_to_idx[ch]] = 1\r\n",
    "        \r\n",
    "    # Iterate over each character in each suffix\r\n",
    "    for k, ch in enumerate(suffix_sentences[i]):\r\n",
    "        # Convert the character to a one-hot encoded vector\r\n",
    "        input_data_suffix[i, k, char_to_idx[ch]] = 1\r\n",
    "\r\n",
    "        # Target data is one timestep ahead and excludes start character\r\n",
    "        if k > 0:\r\n",
    "            target_data[i, k-1, char_to_idx[ch]] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the input layer of the encoder\r\n",
    "encoder_input = Input(shape=(None, len(vocabulary)))\r\n",
    "\r\n",
    "# Create LSTM Layer of size 256\r\n",
    "encoder_LSTM = LSTM(256, return_state = True)\r\n",
    "\r\n",
    "# Save encoder output, hidden and cell state\r\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\r\n",
    "\r\n",
    "# Save encoder states\r\n",
    "encoder_states = [encoder_h, encoder_c]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create decoder input layer\r\n",
    "decoder_input = Input(shape=(None, len(vocabulary)))\r\n",
    "\r\n",
    "# Create LSTM layer of size 256\r\n",
    "decoder_LSTM = LSTM(256, return_sequences=True, return_state = True)\r\n",
    "\r\n",
    "# Save decoder output\r\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\r\n",
    "\r\n",
    "# Create a `Dense` layer with softmax activation\r\n",
    "decoder_dense = Dense(len(vocabulary),activation=\"softmax\")\r\n",
    "\r\n",
    "# Save the decoder output\r\n",
    "decoder_out = decoder_dense(decoder_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build model\r\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\r\n",
    "\r\n",
    "# Compile the model\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\r\n",
    "\r\n",
    "# Print model summary\r\n",
    "model.summary()\r\n",
    "\r\n",
    "# Fit the model\r\n",
    "model.fit(x=[input_data_prefix, input_data_suffix], y=target_data,\r\n",
    "          batch_size=64, epochs=1, validation_split=0.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 3\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create encoder inference model\r\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\r\n",
    "\r\n",
    "# Create decoder input states for inference\r\n",
    "decoder_state_input_h = Input(shape=(256,))\r\n",
    "decoder_state_input_c = Input(shape=(256,))\r\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create encoder inference model\r\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\r\n",
    "\r\n",
    "# Create decoder input states for inference\r\n",
    "decoder_state_input_h = Input(shape=(256,))\r\n",
    "decoder_state_input_c = Input(shape=(256,))\r\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\r\n",
    "\r\n",
    "# Get decoder output and feed it to the dense layer for final output prediction\r\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, initial_state=decoder_input_states)\r\n",
    "decoder_states = [decoder_h , decoder_c]\r\n",
    "decoder_out = decoder_dense(decoder_out)\r\n",
    "\r\n",
    "# Create decoder inference model\r\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states, outputs=[decoder_out] + decoder_states )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pass input prefix to the Encoder inference model and get the states\r\n",
    "inp_seq = input_data_prefix[4:5]\r\n",
    "states_val = encoder_model_inf.predict(inp_seq)\r\n",
    "\r\n",
    "# Seed the first character and get output from the decoder \r\n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\r\n",
    "target_seq[0, 0, char_to_idx['\\t']] = 1  \r\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\r\n",
    "\r\n",
    "# Find out the next character from the Decoder output\r\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\r\n",
    "sampled_suffix_char = idx_to_char[max_val_index]\r\n",
    "\r\n",
    "# Print the first character\r\n",
    "print(sampled_suffix_char)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Insert the generated character from last time to the target sequence \r\n",
    "target_seq = np.zeros((1, 1, len(vocabulary)))\r\n",
    "target_seq[0, 0, max_val_index] = 1\r\n",
    "\r\n",
    "# Initialize the decoder state to the states from last iteration\r\n",
    "states_val = [decoder_h, decoder_c]\r\n",
    "\r\n",
    "# Get decoder output\r\n",
    "decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\r\n",
    "\r\n",
    "# Get most probable next character and print it.\r\n",
    "max_val_index = np.argmax(decoder_out[0,-1,:])\r\n",
    "sampled_suffix_char = idx_to_char[max_val_index]\r\n",
    "print(sampled_suffix_char)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a0f31b5065bb5e11642da8c189697f7b3d952dbfee784a29d8258c5fb037fb8c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}